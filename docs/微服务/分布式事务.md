分布式事务



## 2PC（XA）

数据库ACID事务在分布式场景下无法正确提交与回滚。学术界提出了 2PC 两阶段提交方案，将事务提交拆分成为两阶段过程，称为准备阶段（相当于数据库事务执行但不提交）和提交阶段（统一提交或者回滚）。

XA协议是 2PC 的一个具体实现，引入全局的事务管理器来统一管理准备阶段和提交阶段。Java EE中定义的相应规范称之为 JTA 。XA协议需要资源（比如数据库）支持，具有传统数据库的ACID特性，追求强一致性，也被称为刚性事务（区别于下面的柔性事务）。

但是 2PC（XA）的性能问题比较严重，虽然后来发展了 3PC 协议，但是性能问题没有很好的解决，在现代高性能的分布式场景中几乎没有应用场景。已经被淘汰，现在即使在金融支付等严格的场景中，也更加推荐采用TCC。





## TCC

TCC（Try-Confirm-Cancel）是一种补偿式二阶段提交协议，也借鉴了 2PC 的思想，相比于XA协议由数据库资源实现，TCC完全在业务代码层面实现。

TCC协议将业务逻辑拆分为三个阶段，Try（预留资源，尝试执行，保障一定的隔离性）Confirm（确认执行阶段，直接进行业务处理）Cancel（补偿操作，释放资源）。

TCC相比于XA，不涉及锁和资源的争用，性能更高。TCC属于最终一致性方案，但是在事务隔离性和数据锁定上远强于下面的SAGA和本地消息表方案。在金融和电商支付中，是事实上的行业标准。





## SAGA

跟TCC类似，SAGA也是一种基于补偿机制的分布式事务方案。思路是把一个大的事务分解为多个小事务的集合，初衷是避免大型事务占用数据库资源时间过长。

SAGA的历史比较久远，在现代逐渐发展成将一个分布式环境中的大事务分解为一系列本地事务的设计模式。

相比于TCC方案，SAGA方案没有Try预留资源的步骤，并且由于分为了多个本地小型事务，补偿操作也相比TCC更加容易实现。



> 在脉脉上看到京东某些部门使用的分布式方案：同一个业务操作需要执行多次RPC调用（比如预占库存，使用优惠券，扣减积分），如果其中一个RPC调用失败，比如扣减积分失败，则依次调用回滚接口（回滚积分，回滚优惠券，回滚释放库存），如果其中有回滚失败的，发送MQ，继续回滚其余接口。回滚失败的MQ发送后由对应业务系统接收并且自行兜底。
>
> 这个分布式方案更加类似于SAGA方案，每个RPC调用可以视为一个小的本地事务，如果流程中有RPC调用失败，则调用回滚接口。相当于用补偿操作来撤销事务。并且最后回滚失败，发送MQ消息给对应系统，通过消息队列的特性（不断重试直到最终一致性）来让对应的系统自行兜底。（其中预占资源的操作也借鉴了TCC方案）





## 本地消息表

本地消息表的分布式事务方案，是一种可靠消息队列，尽最大努力交付。主要机制是同步业务新增消息表，任务异步执行，任务异常自动重入。

通过在本地数据库维护一张消息表，业务执行时依靠本地数据库的事务保障业务逻辑和新增消息表这两个操作是一致的。然后再通过异步线程不断查询本地消息表，对未完成的消息业务进行执行，调用失败后也直接重试直到成功，或者失败一定次数后不再重试通知管理员进行人工对账处理。这种分布式事务方案属于最终一致性，一般不具备补偿机制，适用于一定会执行成功的场景，比如发送短信等等。





## 消息队列

消息队列实现的分布式事务跟本地消息表本质上是相同的。主要是将本地消息表对业务侵入的复杂性放到了消息队列中，比如本地自己尝试发送消息，发送失败重试，发送失败后通知管理源。这些在消息队列这个中间件中都是天然实现，消息队列的消息重试和死信队列都直接有这些机制。

重点在于本地消息表的核心思想（业务操作和消息记录的原子性），这个是通过本地数据库事务实现的。在一些消息队列中有对应的原子性保障机制，比如RocketMQ有事务消息的机制，可以实现业务操作和消息记录的原子性。RocketMQ通过半消息和事务回查机制，确保业务事务成功后消息才会被真正提交。





## Seata AT

Seata分布式事务中间件，最常用的AT模式。借鉴了MySQL的事务日志特性，

大致的做法是在业务数据提交时自动拦截所有 SQL，将 SQL 对数据修改前、修改后的结果分别保存快照，生成行锁，通过本地事务一起提交到操作的数据源中，相当于自动记录了重做和回滚日志。如果分布式事务成功提交，那后续清理每个数据源中对应的日志数据即可；如果分布式事务需要回滚，就根据日志数据自动产生用于补偿的逆向 SQL。基于这种补偿方式，分布式事务中所涉及的每一个数据源都可以单独提交，然后立刻释放锁和资源。



分布式事务——凤凰架构

https://icyfenix.cn/architect-perspective/general-architecture/transaction/distributed.html







END

